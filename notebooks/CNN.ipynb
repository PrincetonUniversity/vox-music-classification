{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using existing FVs\n"
     ]
    }
   ],
   "source": [
    "import sys, itertools\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn, time, math\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "if '../tools' not in sys.path:\n",
    "    sys.path.append('../tools')\n",
    "from helper_functions import * \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "X, Y = load_all_fv(3, 3)\n",
    "trX, teX, trY, teY = train_test_split(X, Y, stratify=Y,\n",
    "                                      random_state=1, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trX (900, 32, 1222) trY (900, 10) teX (100, 32, 1222) teY (100, 10)\n"
     ]
    }
   ],
   "source": [
    "print('trX {} trY {} teX {} teY {}'.format(trX.shape, trY.shape, teX.shape, teY.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11776\n",
      "Epoch 01/100 [--------------------------------------------------] 18/18\n",
      "Epoch 02/100 [--------------------------------------------------] 18/18\n",
      "Epoch 03/100 [--------------------------------------------------] 18/18\n",
      "Epoch 04/100 [--------------------------------------------------] 18/18\n",
      "Epoch 05/100 [--------------------------------------------------] 18/18\n",
      "Epoch 5/100 100-sample training error 0.6699999868869781\n",
      "Epoch 06/100 [--------------------------------------------------] 18/18\n",
      "Epoch 07/100 [--------------------------------------------------] 18/18\n",
      "Epoch 08/100 [--------------------------------------------------] 18/18\n",
      "Epoch 09/100 [--------------------------------------------------] 18/18\n",
      "Epoch 10/100 [--------------------------------------------------] 18/18\n",
      "Epoch 10/100 100-sample training error 0.5600000023841858\n",
      "Epoch 11/100 [--------------------------------------------------] 18/18\n",
      "Epoch 12/100 [--------------------------------------------------] 18/18\n",
      "Epoch 13/100 [--------------------------------------------------] 18/18\n",
      "Epoch 14/100 [--------------------------------------------------] 18/18\n",
      "Epoch 15/100 [--------------------------------------------------] 18/18\n",
      "Epoch 15/100 100-sample training error 0.3799999952316284\n",
      "Epoch 16/100 [--------------------------------------------------] 18/18\n",
      "Epoch 17/100 [--------------------------------------------------] 18/18\n",
      "Epoch 18/100 [--------------------------------------------------] 18/18\n",
      "Epoch 19/100 [--------------------------------------------------] 18/18\n",
      "Epoch 20/100 [--------------------------------------------------] 18/18\n",
      "Epoch 20/100 100-sample training error 0.3199999928474426\n",
      "Epoch 21/100 [--------------------------------------------------] 18/18\n",
      "Epoch 22/100 [--------------------------------------------------] 18/18\n",
      "Epoch 23/100 [--------------------------------------------------] 18/18\n",
      "Epoch 24/100 [--------------------------------------------------] 18/18\n",
      "Epoch 25/100 [--------------------------------------------------] 18/18\n",
      "Epoch 25/100 100-sample training error 0.23000001907348633\n",
      "Epoch 26/100 [--------------------------------------------------] 18/18\n",
      "Epoch 27/100 [--------------------------------------------------] 18/18\n",
      "Epoch 28/100 [--------------------------------------------------] 18/18\n",
      "Epoch 29/100 [--------------------------------------------------] 18/18\n",
      "Epoch 30/100 [--------------------------------------------------] 18/18\n",
      "Epoch 30/100 100-sample training error 0.25999999046325684\n",
      "Epoch 31/100 [--------------------------------------------------] 18/18\n",
      "Epoch 32/100 [--------------------------------------------------] 18/18\n",
      "Epoch 33/100 [--------------------------------------------------] 18/18\n",
      "Epoch 34/100 [--------------------------------------------------] 18/18\n",
      "Epoch 35/100 [--------------------------------------------------] 18/18\n",
      "Epoch 35/100 100-sample training error 0.20999997854232788\n",
      "Epoch 36/100 [--------------------------------------------------] 18/18\n",
      "Epoch 37/100 [--------------------------------------------------] 18/18\n",
      "Epoch 38/100 [--------------------------------------------------] 18/18\n",
      "Epoch 39/100 [--------------------------------------------------] 18/18\n",
      "Epoch 40/100 [--------------------------------------------------] 18/18\n",
      "Epoch 40/100 100-sample training error 0.13999998569488525\n",
      "Epoch 41/100 [--------------------------------------------------] 18/18\n",
      "Epoch 42/100 [--------------------------------------------------] 18/18\n",
      "Epoch 43/100 [--------------------------------------------------] 18/18\n",
      "Epoch 44/100 [--------------------------------------------------] 18/18\n",
      "Epoch 45/100 [--------------------------------------------------] 18/18\n",
      "Epoch 45/100 100-sample training error 0.11000001430511475\n",
      "Epoch 46/100 [--------------------------------------------------] 18/18\n",
      "Epoch 47/100 [--------------------------------------------------] 18/18\n",
      "Epoch 48/100 [--------------------------------------------------] 18/18\n",
      "Epoch 49/100 [--------------------------------------------------] 18/18\n",
      "Epoch 50/100 [--------------------------------------------------] 18/18\n",
      "Epoch 50/100 100-sample training error 0.04000002145767212\n",
      "Epoch 51/100 [--------------------------------------------------] 18/18\n",
      "Epoch 52/100 [--------------------------------------------------] 18/18\n",
      "Epoch 53/100 [--------------------------------------------------] 18/18\n",
      "Epoch 54/100 [--------------------------------------------------] 18/18\n",
      "Epoch 55/100 [--------------------------------------------------] 18/18\n",
      "Epoch 55/100 100-sample training error 0.029999971389770508\n",
      "Epoch 56/100 [--------------------------------------------------] 18/18\n",
      "Epoch 57/100 [--------------------------------------------------] 18/18\n",
      "Epoch 58/100 [--------------------------------------------------] 18/18\n",
      "Epoch 59/100 [--------------------------------------------------] 18/18\n",
      "Epoch 60/100 [--------------------------------------------------] 18/18\n",
      "Epoch 60/100 100-sample training error 0.0\n",
      "Epoch 61/100 [--------------------------------------------------] 18/18\n",
      "Epoch 62/100 [--------------------------------------------------] 18/18\n",
      "Epoch 63/100 [--------------------------------------------------] 18/18\n",
      "Epoch 64/100 [--------------------------------------------------] 18/18\n",
      "Epoch 65/100 [--------------------------------------------------] 18/18\n",
      "Epoch 65/100 100-sample training error 0.0\n",
      "Epoch 66/100 [--------------------------------------------------] 18/18\n",
      "Epoch 67/100 [--------------------------------------------------] 18/18\n",
      "Epoch 68/100 [--------------------------------------------------] 18/18\n",
      "Epoch 69/100 [--------------------------------------------------] 18/18\n",
      "Epoch 70/100 [--------------------------------------------------] 18/18\n",
      "Epoch 70/100 100-sample training error 0.0\n",
      "Epoch 71/100 [--------------------------------------------------] 18/18\n",
      "Epoch 72/100 [--------------------------------------------------] 18/18\n",
      "Epoch 73/100 [--------------------------------------------------] 18/18\n",
      "Epoch 74/100 [--------------------------------------------------] 18/18\n",
      "Epoch 75/100 [--------------------------------------------------] 18/18\n",
      "Epoch 75/100 100-sample training error 0.0\n",
      "Epoch 76/100 [--------------------------------------------------] 18/18\n",
      "Epoch 77/100 [--------------------------------------------------] 18/18\n",
      "Epoch 78/100 [--------------------------------------------------] 18/18\n",
      "Epoch 79/100 [--------------------------------------------------] 18/18\n",
      "Epoch 80/100 [--------------------------------------------------] 18/18\n",
      "Epoch 80/100 100-sample training error 0.0\n",
      "Epoch 81/100 [--------------------------------------------------] 18/18\n",
      "Epoch 82/100 [--------------------------------------------------] 18/18\n",
      "Epoch 83/100 [--------------------------------------------------] 18/18\n",
      "Epoch 84/100 [--------------------------------------------------] 18/18\n",
      "Epoch 85/100 [--------------------------------------------------] 18/18\n",
      "Epoch 85/100 100-sample training error 0.0\n",
      "Epoch 86/100 [--------------------------------------------------] 18/18\n",
      "Epoch 87/100 [--------------------------------------------------] 18/18\n",
      "Epoch 88/100 [--------------------------------------------------] 18/18\n",
      "Epoch 89/100 [--------------------------------------------------] 18/18\n",
      "Epoch 90/100 [--------------------------------------------------] 18/18\n",
      "Epoch 90/100 100-sample training error 0.0\n",
      "Epoch 91/100 [--------------------------------------------------] 18/18\n",
      "Epoch 92/100 [--------------------------------------------------] 18/18\n",
      "Epoch 93/100 [--------------------------------------------------] 18/18\n",
      "Epoch 94/100 [--------------------------------------------------] 18/18\n",
      "Epoch 95/100 [--------------------------------------------------] 18/18\n",
      "Epoch 95/100 100-sample training error 0.0\n",
      "Epoch 96/100 [--------------------------------------------------] 18/18\n",
      "Epoch 97/100 [--------------------------------------------------] 18/18\n",
      "Epoch 98/100 [--------------------------------------------------] 18/18\n",
      "Epoch 99/100 [--------------------------------------------------] 18/18\n",
      "Epoch 100/100 [--------------------------------------------------] 18/18\n",
      "Epoch 100/100 100-sample training error 0.0\n",
      "Test error 0.49000000953674316\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import sys\n",
    "if '../tools/TensorFlow-Learn/code' not in sys.path:\n",
    "    sys.path.append('../tools/TensorFlow-Learn/code')\n",
    "from data_set import DataSet\n",
    "import regression\n",
    "\n",
    "train = DataSet(trX, trY)\n",
    "test = DataSet(teX, teY)\n",
    "\n",
    "def batchify(t, wildcard=-1):\n",
    "    t = list(t)\n",
    "    t[0] = wildcard # why doesn't -1 work?\n",
    "    return t\n",
    "\n",
    "# Any batch size on flattened pixel values\n",
    "x = tf.placeholder(\"float\", shape=batchify(trX.shape, wildcard=None))\n",
    "y = tf.placeholder(\"float\", shape=batchify(trY.shape, wildcard=None))\n",
    "\n",
    "x_image = tf.reshape(x, batchify(trX.shape) + [1])\n",
    "\n",
    "def weight_variable(shape):\n",
    "  return tf.Variable(tf.truncated_normal(shape, stddev=0.1))\n",
    "\n",
    "def bias_variable(shape):\n",
    "  return tf.Variable(tf.constant(0.1, shape=shape))\n",
    "\n",
    "def conv2d(x, W):\n",
    "  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_3x3(x):\n",
    "  return tf.nn.max_pool(x, ksize=[1, 3, 3, 1],\n",
    "                        strides=[1, 3, 3, 1], padding='SAME')\n",
    "# TODO max filter instead?\n",
    "\n",
    "W_conv1 = weight_variable([5, 50, 1, 4])\n",
    "b_conv1 = bias_variable([4])\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_3x3(h_conv1)\n",
    "\n",
    "W_conv2 = weight_variable([4, 10, 4, 16])\n",
    "b_conv2 = bias_variable([16])\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_3x3(h_conv2)\n",
    "\n",
    "W_conv3 = weight_variable([2, 2, 16, 128])\n",
    "b_conv3 = bias_variable([128])\n",
    "h_conv3 = tf.nn.relu(conv2d(h_pool2, W_conv3) + b_conv3)\n",
    "h_pool3 = max_pool_3x3(h_conv3)\n",
    "\n",
    "def max_pool_size(orig, filt):\n",
    "    if (orig // filt) * filt == orig: return orig // filt\n",
    "    else: return orig // filt + 1\n",
    "\n",
    "def layer3(x): return max_pool_size(max_pool_size(max_pool_size(x, 3), 3), 3)\n",
    "\n",
    "l3size = 128 * layer3(trX.shape[1]) * layer3(trX.shape[2])\n",
    "W_fc1 = weight_variable([l3size, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "\n",
    "h_pool3_flat = tf.reshape(h_pool3, [-1, l3size])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool3_flat, W_fc1) + b_fc1)\n",
    "\n",
    "# Don't drop out when testing by setting keep_prob to 1.0\n",
    "keep_prob = tf.placeholder(\"float\")\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "output = regression.SoftMax(h_fc1_drop, y)\n",
    "\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(output.cross_entropy)\n",
    "\n",
    "NUM_CORES = 3 # let me do stuff in the background\n",
    "sess = tf.Session(config=tf.ConfigProto(inter_op_parallelism_threads=NUM_CORES,\n",
    "    intra_op_parallelism_threads=NUM_CORES))\n",
    "\n",
    "with sess.as_default():\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "\n",
    "    # TODO add a saver here, this takes forever... Also, time it.\n",
    "    NUM_EPOCHS = 100\n",
    "    UPDATE_COARSENESS = 5\n",
    "    BATCH_SIZE = 50 \n",
    "    for epoch in range(1, NUM_EPOCHS + 1):\n",
    "        for i, batch in enumerate(train.new_epoch(BATCH_SIZE), 1):\n",
    "            tot_batches = train.size // BATCH_SIZE\n",
    "            two_percent_done = i * 50 // tot_batches\n",
    "            print(('\\rEpoch {:02d}/{:02d} [' + two_percent_done * '-' + (50 - two_percent_done) * ' '\n",
    "                   + '] {}/{}').format(epoch, NUM_EPOCHS, i, tot_batches), end='')\n",
    "            sys.stdout.flush()\n",
    "            train_step.run(feed_dict={x: batch[0], y: batch[1], keep_prob: 0.5})\n",
    "        print('')\n",
    "        if epoch == NUM_EPOCHS or UPDATE_COARSENESS and epoch % UPDATE_COARSENESS == 0:\n",
    "            batch = DataSet(*next(train.new_epoch(100)))\n",
    "            err = batch.multiclass_error(x, output.y, y, feed_dict={keep_prob:1.0})\n",
    "            print('Epoch {}/{} 100-sample training error {}'.format(epoch, NUM_EPOCHS, err))\n",
    "\n",
    "    print('Test error {}'.format(test.multiclass_error(x, output.y, y, feed_dict={keep_prob:1.0})))\n",
    "\n",
    "sess.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
